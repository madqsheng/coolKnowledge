# 广播机制

- 概述：
  - NumPy、PyTorch等科学计算库中的数组（张量）操作中常见的特性
  - **使不同尺寸的张量具备可操作性**，自动扩展张量（数组）
- 两种情况：
  1. 两个张量A和B维度相同，但在**某（几）个维度上的尺寸不一样**。但是一定要保证：尺寸不同的那（几）个维度，其中有一个形状是1。比如A形状：(2，1，3，6)，B形状：(1，3，1，6)。虽然在某些维度上尺寸不一样，但是其中之一的形状是1。广播机制会将尺寸不一样的维度上，形状为1扩展到和另外相同的形状。最后两个张量形状相同。
  2. 两个张量的维度不同。将**较小的维度张量**扩展到与**较大维度张量**的维度数相等，办法是通过**在前面添加尺寸为1的维度**来实现。如果两个张量的形状还是不一样，那么继续按照第1种情况扩展。

```python
#情况1


import torch
#张量A是2*3
A=torch.tensor([[0,1,2],
  [3,4,5]])

#张量B是1*3
B=torch.tensor([[1,2,3]])

#torch中一些运算可以触发广播机制，比如+
C=A+B

'''
C=tensor([[1, 3, 5],
        [4, 6, 8]])
1.这里张量A和B尺寸不一样，一般来说是不能用+运算
2.这里的广播机制触发了，就是因为A和B的尺寸虽然不一样，但是在一个维度上尺寸相同，在另一个不同尺寸的维度上A的尺寸是1
3.广播机制使得A会自动采用复制的方法扩展为2*3,实际参与计算中的A=[[1,2,3]，[1,2,3]]
'''

------------------------------------
#广播机制并不只在一个维度上扩展
#张量D是1*3*4
D=torch.arange(12).reshape(1,3,4)

#张量B是4*1*4
E=torch.arange(16).reshape(4,1,4)
'''
D和E向量同样能触发广播机制
1.D和E在第三维度上尺寸相同都是4
2.第一和第二维度上尺寸不同，但是都有1
'''

------------------------------------
# 创建一个形状为(3, 1)的张量
F = torch.tensor([[1], [2], [3]])

# 创建一个形状为(1, 2)的张量
G = torch.tensor([[4, 5]])
'''
F和G向量同样能触发广播机制
1.第一和第二维度不一样，但是这两个维度上都有尺寸1
2.没有其他维度
'''
```

```python
#情况2

import torch

# 创建一个标量（0维张量）
A = 10

# 创建一个形状为(3, 2)的张量
B = torch.tensor([[1, 2], [3, 4], [5, 6]])

# 使用广播机制将标量B添加到张量A的每个元素
C = A + B
'''
A和B向量会触发广播机制
1.维度都不一样
2.小维度张量A扩展到大维度张量B
3.在张量A维度上添加尺寸1，这里需要添加两次，A尺寸：1*1
4.可以继续触发第一种广播机制，A尺寸1*1，B尺寸:3*2
5.第一种情况的广播机制下，A会复制扩展为3*2
6.实际参与计算中的A=[[10,10],[10,10],[10,10]]
最终C=tensor([[11, 12],
             [13, 14],
        	 [15, 16]])
'''
```

# 逐元素相乘

2个要点：

1. 两个**相同形状**的Pytorch张量,对应位置上的元素相乘
2. `torch.mul()`或者`*`来执行逐元素乘法，**两者都支持广播机制**，执行逐元素的乘法操作方面是**完全一样**的，它们具有相同的功能和语义。

```python
import torch

# 创建一个形状为(3, 2)的张量
A = torch.tensor([[1, 2], [3, 4], [5, 6]])

# 创建一个形状为(1, 2)的张量
B = torch.tensor([[2, 3]])

# 使用广播机制进行逐元素乘法
C = torch.mul(A, B)
# 使用广播机制进行逐元素乘法
D = A * B
```

# 内积

3个要点：

1. 也叫**点乘，点积**
2. `torch.dot()` 函数来计算内积
3. **不支持广播机制**

```python
import torch

# 创建两个一维张量
tensor1 = torch.tensor([1, 2, 3])
tensor2 = torch.tensor([4, 5, 6])

# 计算内积
dot_product = torch.dot(tensor1, tensor2)
```

# 矩阵乘法

要点：

1. pytorch提供三个接口API：`torch.mm()`、 `torch.matmul()` 、`@`
2. 其中`@` 运算符和 `torch.matmul()` 函数在执行矩阵乘法操作时是完全等同的。
3. 注意：**`torch.matmul()` 和`@`在计算两个向量（一维张量）的时候，矩阵乘法变成了内积**
4. 注意：**`torch.matmul()` 和`@`支持广播机制**，但是只支持广播机制中**增加维度的情况**，也就是**一维张量和二维张量矩阵乘法的时候**，会在一维张量前面增加尺寸1，从而两个二位张量实现矩阵乘法。
5. 注意：**`torch.matmul()` 和`@`并不支持第一种广播机制**，比如`3*2`矩阵和`1*4`矩阵相乘，并不会将`1*4`矩阵扩展为`2*4`
6. `torch.mm()`是纯粹的矩阵乘法，**不支持广播机制，只支持矩阵（二维张量），并检查尺寸是否复合规则**

```python
import torch

# 创建形状为(3, 2)的张量
A = torch.tensor([[1, 2], [3, 4], [5, 6]])

# 创建形状为(2, 2)的张量
B = torch.tensor([[2, 3], [4, 5]])

# 三种方法，正常的矩阵乘法
result = A @ B
result =torch.matmul(A,B)
result =torch.mm(A,B)

---------------------------------------------------
# 创建两个一维张量
tensor1 = torch.tensor([1, 2, 3])
tensor2 = torch.tensor([4, 5, 6])

# 矩阵乘法变成了内积
dot_product = torch.matmul(tensor1, tensor2)
dot_product = tensor1@tensor2
dot_product = torch.mm(tensor1, tensor2) #报错

----------------------------------
# 创建一个形状为1维张量
A = torch.tensor([2, 3])
# 创建一个形状为(2, 3)的张量
B = torch.tensor([[1, 2,3],[4, 5,6]])

result = A @ B
result =torch.matmul(A,B)
result =torch.mm(A,B)   #报错
'''
触发广播机制
A张量是1维的，扩展为2维，尺寸1*2
'''

--------------------------------------------------
#张量A是3*2
A=torch.arange(6).reshape(3,2)
#张量B是1*4
B=torch.arange(4).reshape(1,4)

#三种情况都在报错
result = A @ B
result =torch.matmul(A,B)
result =torch.mm(A,B)  
```

`torch.bmm()`是批量矩阵乘法，**不支持广播机制**。在张量概念里，**根据张量的维数区分数，向量，矩阵。0维张量是数，1维张量是向量，2维张量是矩阵，3维张量就是批量矩阵**

```python
import torch
#三维张量，第一维代表批量batch
A=torch.arange(6*2).reshape(2,2,3)
B=torch.arange(6*2).reshape(2,3,2)
C=torch.bmm(A,B)
'''

C = tensor([[[ 10,  13],
         	 [ 28,  40]],

        	[[172, 193],
         	 [244, 274]]])
'''
```