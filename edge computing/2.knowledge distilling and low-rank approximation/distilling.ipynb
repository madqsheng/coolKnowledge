{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10d6a66b",
   "metadata": {},
   "source": [
    "# 模型蒸馏教程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63dd410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac73322",
   "metadata": {},
   "source": [
    "## 下载数据集，构建dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ae85a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# %%\n",
    "'''MNIST DataModule'''\n",
    "\n",
    "\n",
    "class MNISTDataModule(L.LightningDataModule):\n",
    "\n",
    "    def __init__(self, dataset_dir, train_batch_size, test_batch_size, train_val_ratio, seed, num_workers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset_dir = dataset_dir #路径\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.train_val_ratio = train_val_ratio #训练验证集比例\n",
    "        self.seed = seed\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            # NOTE ((0.1307,), (0.3081,))，均值是0.1307，标准差是0.3081，由MNIST数据集提供方计算好的\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "        self.dims = (1, 28, 28)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "        # 已经有了就不用下载了\n",
    "        # MNIST(self.data_dir, train=True, download=False)\n",
    "        # MNIST(self.data_dir, train=False, download=False)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        # 分为fit阶段和test阶段\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            # 载入train数据集\n",
    "            mnist_train = MNIST(self.dataset_dir, train=True, download=True, transform=self.transform)\n",
    "            # 划分train数据集的train和val比例\n",
    "            mnist_train_length = len(mnist_train)\n",
    "            train_val = [int(mnist_train_length * ratio) for ratio in self.train_val_ratio]\n",
    "            # 设置seed\n",
    "            generator = torch.Generator().manual_seed(self.seed)\n",
    "\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_train, train_val, generator=generator)\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            self.mnist_test = MNIST(self.dataset_dir, train=False, download=True, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.train_batch_size, num_workers=self.num_workers,persistent_workers=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.test_batch_size, num_workers=self.num_workers,persistent_workers=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.test_batch_size, num_workers=self.num_workers,persistent_workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50bb2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    pass\n",
    "\n",
    "\n",
    "# NOTE:数据集参数\n",
    "dataset_args = Arguments()\n",
    "dataset_args.seed = 42\n",
    "dataset_args.Dataset_Dir = r\"./../../code/torch_data\"\n",
    "dataset_args.train_batch_size = 64\n",
    "dataset_args.test_batch_size = 1000\n",
    "dataset_args.train_val_ratio = (0.8, 0.2)\n",
    "dataset_args.num_workers = 15\n",
    "\n",
    "# %%\n",
    "# 实例化mnist数据集对象\n",
    "mnist = MNISTDataModule(dataset_dir=dataset_args.Dataset_Dir,\n",
    "                        train_batch_size=dataset_args.train_batch_size,\n",
    "                        test_batch_size=dataset_args.test_batch_size,\n",
    "                        train_val_ratio=dataset_args.train_val_ratio,\n",
    "                        seed=dataset_args.seed,\n",
    "                        num_workers=dataset_args.num_workers)\n",
    "mnist.setup()\n",
    "\n",
    "# 实例化dataloaders\n",
    "train_dataloader = mnist.train_dataloader()\n",
    "val_dataloader = mnist.val_dataloader()\n",
    "test_dataloader = mnist.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276ef49d",
   "metadata": {},
   "source": [
    "# 训练教师网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d795ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: : 60160it [00:11, 5031.29it/s]                          \n",
      "testing: 100%|██████████| 10/10 [00:05<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0457, Accuracy: 9853/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: : 60160it [00:09, 6095.76it/s]                          \n",
      "testing: 100%|██████████| 10/10 [00:05<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0380, Accuracy: 9872/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: : 60160it [00:09, 6066.63it/s]                          \n",
      "testing: 100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0350, Accuracy: 9876/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: : 60160it [00:09, 6024.44it/s]                          \n",
      "testing: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0302, Accuracy: 9902/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: : 60160it [00:09, 6089.50it/s]                          \n",
      "testing: 100%|██████████| 10/10 [00:05<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0275, Accuracy: 9911/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "#import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    with tqdm(total=len(train_loader.dataset)) as pbar:\n",
    "        pbar.set_description('training')\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target) #output已经log运算了，本质是交叉熵损失\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % 10 == 0:\n",
    "                pbar.update(len(data)*10)\n",
    "                # print(loss.item())\n",
    "                # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                #    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                #           100. * batch_idx / len(train_loader), loss.item()))\n",
    "                # if args.dry_run:\n",
    "                #    break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='testing'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training settings\n",
    "    #parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    #parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        #help='input batch size for training (default: 64)')\n",
    "\n",
    "    #parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "     #                   help='input batch size for testing (default: 1000)')\n",
    "\n",
    "    #parser.add_argument('--epochs', type=int, default=5, metavar='N',\n",
    "     #                   help='number of epochs to train (default: 14)')\n",
    "\n",
    "    #parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "     #                   help='learning rate (default: 1.0)')\n",
    "\n",
    "    #parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "     #                   help='Learning rate step gamma (default: 0.7)')\n",
    "\n",
    "    #parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "     #                   help='disables CUDA training')\n",
    "\n",
    "    #parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "     #                   help='quickly check a single pass')\n",
    "\n",
    "    #parser.add_argument('--seed', type=int, default=42, metavar='S',\n",
    "     #                   help='random seed (default: 1)')\n",
    "\n",
    "    #parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "     #                   help='how many batches to wait before logging training status')\n",
    "\n",
    "    #parser.add_argument('--save-model', action='store_true', default=True,\n",
    "     #                   help='For Saving the current Model')\n",
    "\n",
    "    #args = parser.parse_args()\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    lr = 1.0\n",
    "    gamma = 0.7\n",
    "    epochs = 5\n",
    "    no_cuda = False\n",
    "    seed = 42\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': batch_size}\n",
    "    test_kwargs = {'batch_size': test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    dataset1 = datasets.MNIST(dataset_args.Dataset_Dir, train=True, download=True,\n",
    "                              transform=transform)\n",
    "    dataset2 = datasets.MNIST(dataset_args.Dataset_Dir, train=False,\n",
    "                              transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train( model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    #if args.save_model:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8df6c6",
   "metadata": {},
   "source": [
    "# 开始蒸馏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b486306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# %%\n",
    "class TeacherNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Network architecture taken from https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "    \n",
    "    98.2% accuracy after 1 epoch\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class StudentNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Naive linear model\n",
    "\n",
    "    92.8% accuracy after 5 epochs, single FC layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(28 * 28, 16)\n",
    "        self.fc2 = nn.Linear(16, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2380252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\"\"\"this file is adapted from\n",
    "    https://github.com/bilunsun/knowledge_distillation  pl_distribution.py\"\"\"\n",
    "# %%\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import lightning as L\n",
    "import lightning.pytorch.callbacks as callbacks\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "class KDMoudle(L.LightningModule):\n",
    "    def __init__(self, teacher, student, learning_rate, temperature, alpha):\n",
    "        super().__init__()\n",
    "\n",
    "        self.teacher = teacher\n",
    "        self.teacher.requires_grad_(False) #冻结teacher模型，不更新参数\n",
    "        self.student = student\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        student_logits = self.student(x)\n",
    "        teacher_logits = self.teacher(x) #都没有softmax层\n",
    "\n",
    "        return student_logits, teacher_logits\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        x, y = batch\n",
    "        student_logits, teacher_logits = self.forward(x)\n",
    "\n",
    "        # # NOTE:第一组：直接用hard_loss训练student网络\n",
    "        # loss = F.cross_entropy(student_logits, y)\n",
    "        #\n",
    "        # # NOTE:第二组：用soft_loss训练student网络\n",
    "        # loss = nn.KLDivLoss()(F.log_softmax(student_logits / self.temperature),\n",
    "        #                       F.softmax(teacher_logits / self.temperature)) * (\n",
    "        #                self.alpha * self.temperature * self.temperature)\n",
    "\n",
    "        # NOTE:第三组：用hard_loss+soft_loss训练student网络\n",
    "        soft_loss = nn.KLDivLoss()(F.log_softmax(student_logits / self.temperature,dim=1),\n",
    "                                   F.softmax(teacher_logits / self.temperature,dim=1)) * (\n",
    "                            self.alpha * self.temperature * self.temperature)\n",
    "        hard_loss = F.cross_entropy(student_logits, y) * (1.0 - self.alpha)\n",
    "        loss = hard_loss + soft_loss\n",
    "\n",
    "        # WHY:student_logits为什么用log_softmax 而 teacher_logits直接用softmax？\n",
    "\n",
    "        self.log(\"student_train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        x, y = batch\n",
    "        student_logits, teacher_logits = self.forward(x)\n",
    "\n",
    "        student_loss = F.cross_entropy(student_logits, y)\n",
    "\n",
    "        student_preds = torch.argmax(student_logits, dim=1)\n",
    "        student_acc = torchmetrics.functional.accuracy(student_preds, y,task='multiclass',num_classes=10)\n",
    "\n",
    "        teacher_preds = torch.argmax(teacher_logits, dim=1)\n",
    "        teacher_acc = torchmetrics.functional.accuracy(teacher_preds, y,task='multiclass',num_classes=10)\n",
    "\n",
    "        self.log(\"student_val_loss\", student_loss, prog_bar=True)\n",
    "        self.log(\"student_val_acc\", student_acc, prog_bar=True)\n",
    "        self.log(\"teacher_val_acc\", teacher_acc, prog_bar=True)\n",
    "\n",
    "        return student_loss\n",
    "\n",
    "    def test_step(self, batch, batch_index):\n",
    "        x, y = batch\n",
    "        student_logits, teacher_logits = self.forward(x)\n",
    "\n",
    "        student_preds = torch.argmax(student_logits, dim=1)\n",
    "        student_acc = torchmetrics.functional.accuracy(student_preds, y,task='multiclass',num_classes=10)\n",
    "\n",
    "        teacher_preds = torch.argmax(teacher_logits, dim=1)\n",
    "        teacher_acc = torchmetrics.functional.accuracy(teacher_preds, y,task='multiclass',num_classes=10)\n",
    "\n",
    "        self.log(\"student_test_acc\", student_acc, prog_bar=True)\n",
    "        self.log(\"teacher_test_acc\", teacher_acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.student.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# callbacks\n",
    "def get_callbacks():\n",
    "    # 监控student_val_loss，不再减小了就停止\n",
    "    early_stopping = callbacks.EarlyStopping(monitor='student_val_loss',\n",
    "                                                            min_delta=1e-4, patience=2,\n",
    "                                                            verbose=False, mode='min')\n",
    "    # checkpoint\n",
    "    model_checkpoint = callbacks.ModelCheckpoint(save_weights_only=True)\n",
    "\n",
    "    # 监控学习率\n",
    "    lr_monitor = callbacks.LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "    return [early_stopping, model_checkpoint, lr_monitor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "729e3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# NOTE:训练参数\n",
    "train_args = Arguments()\n",
    "\n",
    "train_args.learning_rate = 1e-3\n",
    "train_args.max_epochs = 10\n",
    "train_args.temperature = 2\n",
    "train_args.alpha = 0.8\n",
    "# %%\n",
    "# 实例化pl_moudle\n",
    "teacher = TeacherNet()\n",
    "# 载入权重\n",
    "teacher.load_state_dict(torch.load(\"./mnist_cnn.pt\"))\n",
    "student = StudentNet()\n",
    "kd_moudle = KDMoudle(teacher=teacher,\n",
    "                     student=student,\n",
    "                     learning_rate=train_args.learning_rate,\n",
    "                     temperature=train_args.temperature,\n",
    "                     alpha=train_args.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a29e84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: c:\\Users\\admin\\Desktop\\coolShare\\edge computing\\2.knowledge distilling and low-rank approximation\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | teacher | TeacherNet | 1.2 M \n",
      "1 | student | StudentNet | 12.7 K\n",
      "---------------------------------------\n",
      "12.7 K    Trainable params\n",
      "1.2 M     Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.850     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dee00a926fb94051936b9d0a748f60b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ca97ac7b8e4bfb96d769cc70c91c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd42a722d1a3481691a7041467c8c920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee1d521eb9b44479e4781ca7a198d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c5bf2188f043c19640ff87425dc3d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd006726b9b44589a3e5acd5c8aa5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbc9d3c5d194dc7bc5c89b379e5f051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0643b7a01b8646c9b9411fbee5fb1365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8541b3112f774c02948f28db7e62f8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a791bc73234142639c7a92cc01dd082a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecd5d0cd30e4f899d0586571fa7998b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776e0b4c1ccc48b299f179b96741d8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "d:\\anaconda3\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at c:\\Users\\admin\\Desktop\\coolShare\\edge computing\\2.knowledge distilling and low-rank approximation\\lightning_logs\\version_0\\checkpoints\\epoch=9-step=7500.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at c:\\Users\\admin\\Desktop\\coolShare\\edge computing\\2.knowledge distilling and low-rank approximation\\lightning_logs\\version_0\\checkpoints\\epoch=9-step=7500.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d50b503e6a14522a53133d5972efdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "    student_test_acc        0.9462000131607056\n",
      "    teacher_test_acc         0.991100013256073\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'student_test_acc': 0.9462000131607056,\n",
       "  'teacher_test_acc': 0.991100013256073}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    # fast_dev_run=1,  # debug时开启，只跑一个batch的train、val和test\n",
    "    max_epochs=train_args.max_epochs,\n",
    "    callbacks=get_callbacks(),\n",
    "\n",
    "    log_every_n_steps=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "# training\n",
    "trainer.fit(kd_moudle, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "# %%\n",
    "# testing\n",
    "trainer.test(dataloaders=test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
