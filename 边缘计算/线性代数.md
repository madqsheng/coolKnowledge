# 概述

![线性代数1](..\示例图片\线性代数1.jpg)



![线性代数2](..\示例图片\线性代数2.jpg)

- 线性代数的研究包括：
  1. **线性方程组**，求解和分析解
  2. 向量的**线性变换（映射）**
  3. **矩阵**的形式和性质
  4. 求解大规模线性方程组思路，**分块矩阵，矩阵分解**
- **问题1：为什么研究向量？**
  - 向量是生活和工程中普遍存在的一种数据结构，向量的元素对应不同特征。
  - 这种表示方法在数据分析、机器学习和统计学、物理学和工程学中经常被使用
  - 除此之外，**向量运算可以并行**，显著提高计算效率。
- 数学和计算机中的**数据结构**
  - 数据结构是指数据的组织方法
  - **数学领域**中，数据结是一种**逻辑上的形式**，比如**向量，集合，矩阵，张量，图**。实际上，并没有说明**向量具体是怎么存储**的
  - 在==计算机领域==中，数据结构不仅描述了数据的**逻辑组织形式**，还包括数据在**物理层面**的**存储方式和访问方法**。比如：**数组（Array）**、**链表（Linked List）**、**栈（Stack）**、**队列（Queue）**、**树（Tree）**、**图（Graph）**、**堆（Heap）**、**散列表（Hash Table）**、**链表（Linked List）**、**字典（Dictionary）**、**栈（Stack）**、**位图（Bit Array）**等

# 1. 线性方程组

- 线性

  - **两个对象之间的一种关系**。
  - 线性的特征表现为：**比例关系，简单相加关系**
  - 比如两个变量x和y，存在一种关系表达式：y=ax+b，就可以说，**x和y之间是线性关系**
  - 线性的概念贯穿始终

- 线性方程

  - 包含未知数变量x~1~,x~2~,⋯,x~n~的一个**线性方程**如下所示：
    $$
    a_{1}x_{1}+a_{2}x_{2}+\cdots+a_{n}x_{n}=b
    $$

- **线性方程组**是由一个或多个包含相同变量x~1~,x~2~,⋯,x~n~的线性方程组成

## 1.1 （行）阶梯形矩阵

- 系数矩阵

- 增广矩阵
  $$
  \left[
   \begin{matrix}
     \pmb{a_1} & \pmb{a_2} & .. & \pmb{a_n} & \pmb{b} \\
    \end{matrix}
    \right]
  $$
  
- 初等行变换

- 简化阶梯形矩阵

  - **定理：每个矩阵行等价于唯一的简化阶梯形矩阵**
  - 先导元素：非零行中最左边的非零元素
  - 主元位置：阶梯形矩阵中先导元素1的位置
  - 主元列：阶梯形矩阵含有主元位置的列

## 1.2 向量方程

- 列向量（向量）：矩阵的某一列，表示一组有序数

- 线性组合

- 向量方程

  - 包含未知数变量x~1~,x~2~,⋯,x~n~的方程：

  $$
  x_{1}\pmb{a_{1}}+x_{2}\pmb{a_{2}}+\cdots+x_{n}\pmb{a_{n}}=\pmb{b}
  $$

## 1.3 矩阵方程

- 矩阵与向量的积

  - 如*A*是*m x n*矩阵，它的各列为**a~1~**,**a~2~**,⋯,**a~n~**，**x**是**R^n^**中的向量，则*A*与**x**的积（记为*A***x**）为：
    $$
    A\pmb{x}=
    \left[
     \begin{matrix}
       \pmb{x_1} & \pmb{x_2} & ... & \pmb{x_n} \\
      \end{matrix}
      \right] 
      \left[
     \begin{matrix}
       x1 \\
       x2\\
       ... \\
       xn
      \end{matrix}
      \right]=x_{1}\pmb{a_{1}}+x_{2}\pmb{a_{2}}+\cdots+x_{n}\pmb{a_{n}}
    $$
    

- 矩阵方程
  $$
  A\pmb{x}=\pmb{b}
  $$

## 1.4 线性无关与相关

- 定义：

  - Rn中的一组向量{**v~1~**,...,**v~p~**}，对于向量方程：
    $$
    x_1\pmb{v_1} + x_2\pmb{v_2} + ... + x_p\pmb{v_p} = \pmb{0}
    $$
    仅有平凡解（零解），则这组向量称为**线性无关**

  - 如果向量方程有非零解，则这组向量称为**线性相关**

- 理解

  - 线性相关性是**一个向量集中的向量之间**的关系，通常是用来描述**向量集内部的关系**。
  - 只能说，**向量组A线性相关或无关，实质是说向量组A中的向量满足上面的性质**。不存在描述，**向量A和向量B线性相关，向量a和向量b线性相关**
  - 线性无关比线性相关更重要

- **矩阵各列的线性无关**，等价于
  $$
  对于矩阵A，矩阵方程A\pmb{x}=\pmb{0},仅有平凡解
  $$

## 1.4.1 定理：向量组线性相关

- **若一个向量组的向量个数超过每个向量的元素个数，那么这个向量组必线性相关**

# 2. 线性变换

- **变换（或者映射、函数）**

  - 对象：**向量**

  - 定义域，值域：**向量空间**

  - 定义：
    $$
    T:R^n\rightarrow R^m
    $$
    T代表变换规则，它把R^n^中的每个向量**x**对应到R^m^中的一个向量T(**x**)，R^n^和R^m^分别是定义域和值域

- **矩阵变换**

  - 定义：
    $$
    \pmb{x} \rightarrow A\pmb{x}
    $$
    对R^n^中的每个向量**x**，T(**x**)由A**x**计算得到

- **线性变换**

  - 线性的条件：
    1. 对T的定义域中的一切向量**u**,**v**，T(**u** + **v**) = T(**u**) + T(**v**)
    2. 对T的定义域中的一切向量**u**和数c，T(c**u**) = cT(**u**)

## 2.1 由线性变换求矩阵

- **定理**：设T:R^n^→R^m^为线性变换，则存在唯一的矩阵A，使得对R^n^中的一切**x**，满足：
  $$
  T(\pmb{x})=A\pmb{x}
  $$

  - 换句话说：从R^n^到R^m^的**每一个线性变换**，实际上都是一个**矩阵变换**:**x**→A**x**

- 由线性变换求矩阵

  T:R^n^→R^m^为线性变换，对应的矩阵A是*m x n*：
  $$
  A=
  \left[
   \begin{matrix}
     T(\pmb{e_1}) & T(\pmb{e_2}) & ... & T(\pmb{e_n}) \\
    \end{matrix}
    \right]
  $$
  其中**e~i~**是R^n^中的单位矩阵I~n~的第i列

  - 意义：**可以直接根据线性变换的几何意义直接得到变换矩阵**
    - **e~i~**是固定的
    - T(**e~i~**)可以根据变换的几何意义给出
  - 常见几何变换属于线性变换：
    1. **对称**
    2. **收缩与拉伸**
    3. **剪切**
    4. **投影**
    5. **旋转**
  - 前提：是线性变换
  - 注意：**平移不是线性变换**

## 2.2 定理

- 如果T:R^n^→R^m^为线性变换，下面3结论等价：
  1. **T变换是一对一**
  2. 矩阵方程A**x**=**0**，仅有平凡解
  3. 矩阵A的列线性无关

# 3.矩阵

## 3.1 矩阵乘法

- A(B**x**)认为是有x经复合映射所得：
  $$
  A(B\pmb{x})=(AB)\pmb{x}
  $$

- 矩阵乘法定义：

  A是*m x n*矩阵，B是*n x p*矩阵，B的列是b~1~,...,b~p~，则AB是*m x p*矩阵，它的各列是A**b~1~**,...,A**b~p~**，即：
  $$
  AB=A
  \left[
   \begin{matrix}
     \pmb{b_1} & \pmb{b_2} & ... & \pmb{b_p} \\
    \end{matrix}
    \right]
    =\left[
   \begin{matrix}
     A\pmb{b_1} & A\pmb{b_2} & ... & A\pmb{b_p} \\
    \end{matrix}
    \right]
  $$

## 3.2 矩阵运算性质

![矩阵运算性质](..\示例图片\矩阵运算性质.png)

- 转置矩阵性质

  ![转置矩阵性质](..\示例图片\转置矩阵性质.png)

  **如干矩阵的乘积的转置等于它们的转置的乘积，但相乘的顺序相反**

## 3.3 逆矩阵

- 定义：A是*n x n*矩阵，若存在一个*n x n*矩阵C使
  $$
  CA=I\quad且\quad AC=I
  $$
  其中I是*n x n*单位矩阵，那么就称A是**可逆矩阵**，A和C互为各自的**逆矩阵**

  - **奇异矩阵**（不可逆）
  - **非奇异矩阵**（可逆）

### 3.3.1 **定理**

1. 如果A是可逆矩阵，则对R^n^中的**任何**向量**b**，矩阵方程A**x**=**b**有**唯一解**

### 3.3.2 逆矩阵的性质

![逆矩阵性质1](..\示例图片\逆矩阵性质1.png)

![逆矩阵性质2](..\示例图片\逆矩阵性质2.png)

**如干个n x n 可逆矩阵的积也是可逆的，其逆等于这些矩阵的逆按相反顺序的乘积**

### 3.3.3 初等矩阵

- 定义：**单位矩阵**进行一次**初等行变换**，就得到**初等矩阵**

- 定理：
  1. 若对*m x n*矩阵A进行**某种初等行变换**，所得矩阵可以写成**EA**，其中**E是*m x m*矩阵**，是由单位矩阵I~n~进行**同一行变换**所得
  2. 每个**初等矩阵E都是可逆**
  3. 结论等价：
     - nx n*矩阵A可逆
     - A行等价于单位矩阵I~n~

### 3.3.4 求逆矩阵的思路

1. 把增广矩阵[*A*   *I*]进行行化简（初等行变换），如A行等价于I，则[*A*   *I*]行等价于[*I*   *A^-1^*]

2. 特别地，2 x 2 矩阵
   $$
   A=
   \left[
    \begin{matrix}
      a & b\\
      c & d\\
     \end{matrix}
     \right]
   $$
   如果ad - bc ≠ 0，则A可逆，且
   $$
   A^{-1}=\frac{1}{ad - bc}
   \left[
    \begin{matrix}
      d & -b\\
      -c & a\\
     \end{matrix}
     \right]
   $$
   

### 3.3.5 可逆矩阵等价定理

![可逆矩阵定理](..\示例图片\可逆矩阵定理.png)

### 3.5.6 相似（矩阵）

- **A相似于B**

  假如A和B是*n x n*矩阵，如果存在可逆矩阵P，使得P^-1^AP=B，称**A相似于B**

- **相似变换**

  把A变换为P^-1^AP的变换

## 3.4 分块矩阵

- 分块矩阵运算

  1. 加法、标量乘法

  2. 分块矩阵乘法

     ![分块矩阵运算](..\示例图片\分块矩阵运算.png)

### 3.4.1 定理：矩阵乘法列行展开

![矩阵列行展开](..\示例图片\矩阵列行展开.png)



## 3.5 LU矩阵分解

[LU分解：矩阵初等行变换阶梯型分解](#9.1 LU分解：初等行变换分解)

## 3.6 R^n^的子空间

[详细](#5. 向量空间)

- 定义：

  ![子空间定义](..\示例图片\子空间定义.png)

  - 理解：
    1. 向量空间**并非物理上的集合**，而是一种**逻辑上的集合**
    2. 比如a和b向量组成一个向量空间，并不是说这向量空间只有a和b。还包括它们的**线性组合**
    3. 空间是一种逻辑概念，一般都是**无穷的**
    4. **向量空间**也叫**线性空间**，两者概念等同

- 零子空间：仅含有零向量

### 3.6.1 矩阵的列空间

- 定义：矩阵A的**列空间**是A的各列的线性组合的集合，记作**Col A**
- **矩阵A的零空间**：齐次方程A**x**=**0**的**所有解的集合**，记作**Nul A**

### 3.6.2 子空间的基

- 定义：R^n^中的子空间H的一组基是H中的一个**线性无关集**，它生成H
- 定理：矩阵A的**主元列**构成A的**列空间的基**

## 3.7 维数与秩

### 3.7.1 坐标向量

- 定义：

  ![坐标向量](..\示例图片\坐标向量.png)

### 3.7.2 子空间的维数

- 定义：非零子空间H的维数（用**dim H**表示）是H的**任意一个基的向量个数**。零子空间的维数定义为0
- 理解：
  - **向量和向量空间的维度不是物理学或空间几何上的维度**，它们都是**抽象的逻辑概念**。
  - **在物理空间意义上，向量是1维的，向量空间是2维**
  - **向量的维度**是指它所包含的**分量或元素的数量**
  - **向量空间的维度**是指该向量空间中的**基的向量个数**

### 3.7.3 矩阵的秩

- 定义：矩阵A的秩（记为rank A）是A的**列空间的主元列的个数**
- 定理：如果一矩阵有n列，则**rank + dim Nul A = n**

### 3.7.4 定理：逆矩阵的秩

![逆矩阵的秩](..\示例图片\逆矩阵的秩.png)

# 4. 行列式

- 定义
- 余因子展开式

## 4.1 定理：三角矩阵

- 若A是三角矩阵，则det A（矩阵A的行列式）等于A的**主对角线上的元素的乘积**
- 注意：
  - **三角矩阵**和**对角矩阵**并不是一个概念
  - 三角矩阵：所有**主对角线**以下（或以上）的元素都为零。分为**上三角矩阵**和**下三角矩阵**
  - 对角矩阵：除了**主对角线**上的元素之外，所有其他元素都为零。

## 4.2 行列式的性质

1. 行列式的初行变换效果

   ![行变换的行列式](..\示例图片\行变换的行列式.png)

2. **行列式的初等行变换和初等列变换具有相同效果**

   如果A为*n x n*矩阵，则**det A ^T^ = det A**

3. 行列式与矩阵乘积：

   如果A和B都是*n x n*矩阵，则**det AB = (det A)(det B)**

## 4.3 定理：行列式与可逆性

- 思路：初等变换、线性无关

  ![行列式与可逆的思路](..\示例图片\行列式与可逆的思路.png)

- 两个结论等价：
  1. **方阵A可逆**
  2. **det A ≠ 0**

## 4.4 克拉默法则

![克拉默法则](..\示例图片\克拉默法则.png)

- 伴随矩阵

### 4.4.1 求矩阵的逆的一种思路

- 定理：如果A是一个可逆的*n x n*矩阵，则
  $$
  A^{-1}=\frac{1}{\text{det A}}\text{adj A}
  $$

  - 理解
    1. 此公式可以使得我们不用实际计算初A^-1^就可以推导出A^-1^的性质
    2. 特殊情况外，一般不选择这种方法计算矩阵的逆
    3. 克拉默法则是一个理论工具，它可以用来研究当**b**或者A中某元素改变时，A**x**=**b**的解如何变化

## 4.5 矩阵行/列变换求行列式

![行变换求行列式](..\示例图片\行变换求行列式.png)

- 初等行/列变换——**不要行倍乘的阶梯矩阵**
  1. 行/列交换——符号变换
  
  2. 行/列代替——不变
  
  3. **不要行/列倍乘**
  
  4. **行列式的初等行变换和初等列变换具有相同效果**
  
     如果A为*n x n*矩阵，则**det A ^T^ = det A**

# 5. 向量空间

## 5.1 向量空间

- 定义

  ![向量空间](..\示例图片\向量空间.png)

## 5.2 子空间

[同R^n^的子空间](##3.6 R^n^的子空间)

- 定义

  ![子空间](..\示例图片\子空间.png)

## 5.3 Span{}集合生成的子空间

- 定义

  ![集合子空间](..\示例图片\集合子空间.png)

## 5.4 矩阵的零空间与列空间

- [零空间 Nul A](###3.6.1 矩阵的列空间)

  齐次方程A**x**=**0**的**所有解的集合**，记作**Nul A**
  $$
  Nul\text{ A}=\left\{
   \begin{matrix}
    \pmb{x}:\pmb{x}\epsilon R^n,A\pmb{x}=\pmb{0} \\
    \end{matrix}
    \right\}
  $$

- [列空间Col A](###3.6.1 矩阵的列空间)

  *m x n*矩阵A的**列空间**是由A的各列的所有线性组合组成的集合，记作**Col A**

- 列空间和零空间对比

  ![列空间和零空间对比](..\示例图片\列空间和零空间对比.png)

## 5.5 线性变换的核与值域

- [线性变换定义](#2. 线性变换)

  ![列空间和零空间对比](..\示例图片\向量空间的线性映射.png)

- 线性变换的核（或叫零空间）

- 值域

## 5.6 基

[子空间的基](###3.6.2 子空间的基)

- 线性无关

- 定义

  ![基](..\示例图片\基.png)

  - **标准基**

### 5.6.1 定理

1. 生成集定理

   ![生成集定理](..\示例图片\生成集定理.png)

2. 列空间的基

   矩阵A的**主元列**构成**Col A**的一个基

### 5.6.2 坐标系

[坐标向量](###3.7.1 坐标向量)

- 唯一表示定理：

  ![唯一表示定理](..\示例图片\唯一表示定理.png)

- 定义：

  假设**β**={**b~1~**,...,**b~n~**}是***V***的一个基，**x**在***V***中，满足
  $$
  \pmb{x}=c_1\pmb{b_1}+...+c_n\pmb{b_n}
  $$
  的权c~1~,...,c~n~，叫**x**相对于基**β**的坐标，或者叫**x**的**β**-坐标

  - **坐标向量**

    **x**相对于基**β**的坐标，组成的向量叫**x**的坐标向量，或者叫x的**β**-坐标向量
    $$
    [\pmb{x}]_β=
    \left[
     \begin{matrix}
       c_1\\
       \vdots \\
       c_n\\
      \end{matrix}
      \right]
    $$
  
- **坐标映射**
    $$
    \pmb{x}\rightarrow [\pmb{x}]_β
    $$
  
- **坐标变换矩阵**
    $$
    P_β=
    \left[
     \begin{matrix}
       \pmb{b_1} & \pmb{b_2} & \cdots\ \pmb{b_n}\
      \end{matrix}
      \right]
    $$
    坐标变换矩阵是基组成，**必可逆**，因此求坐标向量：
    $$
    [\pmb{x}]_β=P_β^{-1}\pmb{x}
    $$
    
- 定理：坐标映射必一对一

  ![坐标映射一对一](..\示例图片\坐标映射一对一.png)

- 坐标变换矩阵

  ![坐标变换矩阵](..\示例图片\坐标变换矩阵.png)

## 5.7 向量空间的维数

[子空间的维数](###3.7.2 子空间的维数)

- 定理：

  1. **多余基数量的向量集必线性相关**

     ![大于基数量的向量集](..\示例图片\大于基数量的向量集.png)

  2. **基数量相同**

     ![基数量相同](..\示例图片\基数量相同.png)

- 定义

  ![向量空间的维数](..\示例图片\向量空间的维数.png)

- Col A和Nul A的维数

  ![Col A和Nul A的维数](..\示例图片\Col A和Nul A的维数.png)

## 5.8 矩阵的秩

[矩阵的秩](###3.7.3 矩阵的秩)

- 行空间

  ![行空间](..\示例图片\行空间.png)

- 定理：行等价，则行空间相同，行空间的基

  ![行空间](..\示例图片\行空间定理.png)

- 矩阵的秩：矩阵A的秩（记为rank A）是A的**列空间的主元列的个数**

### 5.8.1 秩定理

![秩定理](..\示例图片\秩定理.png)

### 5.8.2 思路：秩、基、方程组的解

对于*m x n*矩阵A，如果**rank A = n**，矩阵A的列空间就是R^n^的基，矩阵方程**Ax=b**必有解

### 5.8.3 逆矩阵的秩

![逆矩阵的秩](..\示例图片\逆矩阵的秩.png)

## 5.9 差分方程

# 6. 特征值与特征向量

- 从线性变换观察的现象

  ![线性变换的一个观察](..\示例图片\线性变换的一个观察.png)

- 定义：

  ![特征值和特征向量定义](..\示例图片\特征值和特征向量定义.png)

  - **特征空间**

    方程的(A-λ*I*)**x**=**0**所有**非平凡解**的集合，称为A的对应于λ的特征空间

## 6.1 定理：三角矩阵特征值

**三角矩阵的主对角线的元素是其特征值**

## 6.2 定理：特征向量线性无关

- 注意：
  1. **特征值相异**，其对应的特征向量才线性无关
  2. **定理里并没有规定矩阵的特征值的数量**
  3. **同一特征值对应的多个特征向量，并不一定线性相关**

![特征向量线性无关](..\示例图片\特征向量线性无关.png)

## 6.3 可逆、行列式、特征值

- 思路：

  1. 矩阵A存在特征值和特征向量
  2. 方程(A-λ*I*)**x**=**0**有非平凡解
  3. 矩阵(A-λ*I*)不可逆
  4. 矩阵(A-λ*I*)的行列式等于0

- 下一步：如何求矩阵**(A-λ*I*)的行列式**？

  初等行变换中的**行交换和行倍数相加**，必可以得到**行阶梯矩阵（上三角矩阵）**

## 6.4 可逆矩阵的特征值，行列式

- P成立，当且仅当Q：如果P则Q，且如果Q则P

  ![可逆矩阵的行列式和特征值](..\示例图片\可逆矩阵的行列式和特征值.png)

  - 可逆等价于
    1. 行列式不等于0
    2. 0不是矩阵的特征值

## 6.5 特征方程

$$
det (A-λI)=0
$$

数λ是*n x n*矩阵A的特征值的充要条件是，λ是以上特征方程的根

- 特征多项式
  $$
  det (A-λI)
  $$

## 6.6 定理：相似矩阵的特征值

[相似矩阵](###3.5.6 相似（矩阵）)

![相似矩阵的特征值](..\示例图片\相似矩阵的特征值.png)

## 6.7 特征值分解：判断对角矩阵相似

[特征值分解](##9.3 特征值分解)

- 相似为对角矩阵的动机：
  1. **相似矩阵的特征值相同**
  2. **对角矩阵的特征值显而易见**

- 判断原则：**矩阵A有n个线性无关的特征向量**

  ![相似对角化](..\示例图片\相似对角化.png)

### 6.7.1 矩阵特征值是否相异

1. 有**n个相异特征值**的*n x n*矩阵**可对角化**

2. 特征值不都相异的情况：

   *n x n*矩阵**可对角化**的充要条件是，**所有不同特征空间的维数之和为n**。即每个特征值λ~i~的**特征空间的维数**等于**λ~i~的代数重数**

   ![特征值不都相异](..\示例图片\特征值不都相异.png)

# 7. 向量内积

- **内积**，也叫**点积**，**点乘**

  ![向量内积](..\示例图片\向量内积.png)

  - 注意：**向量的内积是一个数**

- 内积性质

  ![内积性质](..\示例图片\内积性质.png)



## 7.1 向量长度

- 定义

![向量长度](..\示例图片\向量长度.png)

- **单位向量**：向量长度为1的向量

## 7.2 向量距离

![向量距离](..\示例图片\向量距离.png)

## 7.3 向量正交

- 定义

  如果两个向量的内积等于0：
  $$
  \pmb{u}·\pmb{v}=0
  $$
  则R^n^中的两个向量**u**和**v**是（相互）**正交**

### 7.3.1 定理：向量正交充要条件

![毕达哥拉斯正交定理](..\示例图片\毕达哥拉斯正交定理.png)

- 正交补

  1. 如果向量**z**与R^n^的子空间*W*的任意向量都正交，则称**z**正交于*W*。
  2. 与子空间*W*正交的向量**z**的**全体组成的集合**称为*W*的**正交补**，记作***W*^⊥^**

- 定理：列空间和行空间的正交补

  ![列空间和行空间的正交补](..\示例图片\列空间和行空间的正交补.png)

### 7.3.2 正交基

- 定义：基且正交

![正交基](..\示例图片\正交基.png)

- 定理：向量在正交基下的权

  ![正交基的权](..\示例图片\正交基的权.png)

### 7.3.3 正交投影

![正交投影图示](..\示例图片\正交投影图示.png)

- 正交投影：

  目标是向量**y**，给定一个向量**u**，求两个向量满足：

  1. $$
     \pmb{y}=\pmb{\hat{y}}+\pmb{z}
     $$

  2. 向量$\pmb{\hat{y}}$是给定向量**u**的倍数
  
  3. 向量**z**与向量**u**正交
  
  称向量$\pmb{\hat{y}}$为向量**y**在向量**u**上的**正交投影**，记作proj~L~  **y**。向量**z**为**y**与**u**正交的**分量**
  
  ![正交投影](..\示例图片\正交投影.png)

#### 7.3.3.1 定理：矩阵的列向量单位且正交

1. ***m x n***矩阵的列向量满足单位向量，互相正交的充要条件

   ![列向量单位正交](..\示例图片\列向量单位正交.png)

2. 列向量单位且正交的矩阵性质

   ![列向量单位且正交的矩阵性质](..\示例图片\列向量单位且正交的矩阵性质.png)

- 正交矩阵：可逆矩阵U，且满足**U^-1^=U^T^**

  推断：正交矩阵的列向量正交且是单位矩阵

#### 7.3.3.2 正交分解定理

![正交分解定理](..\示例图片\正交分解定理.png)

#### 7.3.3.3 最佳逼近定理

![最佳逼近定理](..\示例图片\最佳逼近定理.png)

#### 7.3.3.4 定理：求向量空间的正交投影

![向量空间的正交投影](..\示例图片\向量空间的正交投影.png)

### 7.3.4 格拉姆-施密特方法：构造任何非零子空间的正交基

![格拉姆施密特方法](..\示例图片\格拉姆施密特方法.png)

- 定理：[QR分解](##9.2 QR分解：矩阵列线性无关求正交基分解) 

## 7.4 向量空间称为内积空间

- 内积空间

  1. 是向量空间的一种
  2. 向量空间可以称为内积空间需要满足条件：
     - 定义一种**向量内积运算**<**u**,**v**>
     - 内积运算又满足一些性质

  ![内积空间](..\示例图片\内积空间.png)

# 8. 对称矩阵

- 对称矩阵：**方阵**，满足**A^T^=A**
- 对称矩阵很常见，**理论完美**

## 8.1 定理：对称矩阵的特征向量

![对称矩阵的特征向量](..\示例图片\对称矩阵的特征向量.png)

## 8.2 对称矩阵特征值分解：正交对角化

- *n x n*矩阵A分解：**A=PDP^-1^=PDP^T^**
  1. 本质是特征值分解，只不过加了一个分解限制，P还是**正交矩阵**
  2. 顺着特征值分解的思维，P矩阵依然是A矩阵的**n个线性无关特征向量**
  3. 再加上上面的限制，P是正交矩阵，推出**P的列向量是正交且单位向量**
  4. 总结：P的列向量不仅是**A矩阵的n个线性无关的特征向量**，并且是**单位正交向量**

### 8.2.1 定理：矩阵正交对角化充要条件

![矩阵可正交对角化条件](..\示例图片\矩阵可正交对角化条件.png)

### 8.2.2 谱定理：对称矩阵的性质

![对称矩阵性质](..\示例图片\对称矩阵性质.png)

### 8.2.3 谱分解：对称矩阵的特征值分解过程

![谱分解](..\示例图片\谱分解.png)

1. 对称矩阵必有**n个线性无关的特征向量**，**n个特征值**
2. 对n个特征向量进行施密特方法，求**n个单位正交基**，就是矩阵P

## 8.3 二次型

- 描述二次型：

  1. 没有明确定义
  2. 二次型是指**矩阵和向量相关的数学表达式**
  3. 进一步，**二次型是多项式**，但**所有项是二次**

- **关于二次型的矩阵**

  1. 表达式Q(**x**)=**x**^T^A**x**

  2. 矩阵A是对称矩阵，尺寸*n x n*

     矩阵A称为关于二次型的矩阵

### 8.3.1 二次型的变量代换

- 内容：

  如果**x**表示R^n^中的向量变量，**变量代换**有如下等式：
  $$
  \pmb{x}=P\pmb{y} \quad 或 \quad \pmb{y}=P^{-1}\pmb{x}
  $$
  其中P矩阵可逆，**y**向量是R^n^中一个新的向量变量

- 用变量变换处理二次型
  $$
  \pmb{x}^TA\pmb{x}=(P\pmb{y})^TA(P\pmb{y})=\pmb{y}^TP^TAP\pmb{y}=\pmb{y}^T(P^TAP)\pmb{y}=\pmb{y}^TD\pmb{y}
  $$

  - 如果**A是对称矩阵**，**P^T^AP=D**正好是**正交对角化**

### 8.3.2 主轴定理：对称矩阵存在正交变量代换

![主轴定理](..\示例图片\主轴定理.png)

### 8.3.3 二次型的分类：正负定

![二次型分类](..\示例图片\二次型分类.png)

### 8.3.4 二次型与特征值

![二次型与特征值](..\示例图片\二次型与特征值.png)

### 8.3.5 条件优化

### 8.3.6 [奇异值分解](##9.4 奇异值分解)

- 动机
  1. 特征值分解，矩阵对角化，有个条件，矩阵A有n个线性无关的特征向量
  2. 还有一个条件，矩阵A是方阵，那如果是*m x n*矩阵，有办法分解吗？
  3. 给出结论：A=QDP^-1^对任意*m x n*矩阵A都是可能的，这就是**奇异值分解**

#### 8.3.6.1 m x n矩阵的奇异值

- 矩阵A的奇异值其实是由A^T^A定义：
  1. 矩阵A尺寸*m x n*，研究矩阵A^T^A，是**对称矩阵**，意味着可以**正交对角化**
  2. 研究向量A**v~i~**
- 结论：
  1. 矩阵A^T^A尺寸*n x n*，对称矩阵，**所有特征值都是非负**
  2. 矩阵A的奇异值是**矩阵A^T^A的特征值的平方根**
  3. 矩阵A的**奇异值**是向量A**v~i~**的长度

![矩阵的奇异值](..\示例图片\矩阵的奇异值.png)

#### 8.3.6.2 定理：非零奇异值到矩阵秩和列空间正交基

![非零奇异值到矩阵秩](..\示例图片\非零奇异值到矩阵秩.png)

#### 8.3.6.3 奇异值分解内容

![奇异值分解](..\示例图片\奇异值分解.png)

### 8.3.7 定理：可逆矩阵的非零奇异值

![可逆矩阵的非零奇异值](..\示例图片\可逆矩阵的非零奇异值.png)

# 9. 矩阵分解

## 9.1 LU分解：初等行变换分解

- LU分解 **A=LU** 要点：
  1. 要求：矩阵A可逆，矩阵A行列式不等于0，矩阵A的各列线性无关，三者等价
  2. 矩阵L是一个**下三角矩阵**（**Lower Triangular matrix**，简称L），**主对角线上的元素都为1**
  3. 矩阵U是一个**上三角矩阵**（**Upper Triangular matrix**，简称U），**主对角线上的元素不一定为1**
- 如何求解矩阵L和U：
  1. **高斯消元法**
  2. **Doolittle分解**

## 9.2 QR分解：矩阵列线性无关求正交基分解

- QR分解 **A=QR** 要点：

  1. 要求：*m x n*矩阵A的**列线性无关**
  2. 矩阵Q尺寸*m x n*，其列是Col A的**标准正交基**
  3. 矩阵R尺寸*n x n*，是一个**上三角可逆矩阵**，对角线的元素是正数

  - 求R矩阵：
    $$
    R=IR=Q^TQR=Q^T(QR)=Q^TA
    $$
    注意：Q是**标准正交基的充要条件**是:     $Q^TQ=I$

![QR分解定义](..\示例图片\QR分解定义.png)

## 9.3 特征值分解：对角化

- 特征值分解**A=PDP^-1^**要点：

  1. 要求：***n x n*矩阵**A有**n各线性无关的特征向量**
  2. 矩阵P尺寸*n x n*，**可逆**，并且**其列向量是A的n个线性无关的特征向量**
  3. 矩阵D尺寸*n x n*，**对角矩阵**，且主对角线上的元素分别是A的对应于P中特征向量的**特征值**

- 拆解条件：***n x n*矩阵**A有**n各线性无关的特征向量**

  1. 矩阵A有**n个相异特征值**，必然有n个线性无关的特征向量，必然可以特征值分解，即可对角化

  2. 特征值不都相异的情况：

     *n x n*矩阵**可对角化**的充要条件是，**所有不同特征空间的维数之和为n**。即每个特征值λ~i~的**特征空间的维数**等于**λ~i~的代数重数**

     ![特征值不都相异](..\示例图片\特征值不都相异.png)

### 附加条件：正交对角化

- 谱分解/正交对角化**A=PDP^-1^=PDP^T^**要点：
  1. 要求：A是对称矩阵，尺寸*n x n*
  2. P矩阵，尺寸*n x n*，是正交矩阵，意味着**P^-1^=P^T^**，P列向量是**单位矩阵且正交**
  3. 矩阵D，尺寸*n x n*，**对角矩阵**，且主对角线上的元素分别是A的对应于P中特征向量的**特征值**
- 分解方法
  1. 先求特征值和特征向量
  2. 格拉姆-施密特方法求特征向量的正交基，标准化，得到矩阵P

## 9.4 奇异值分解

[详细奇异值分解](###8.3.6 奇异值分解)

- 奇异值分解**A=UΣV^T^**要点：

  1. 对矩阵A没有要求，尺寸是任意的*m x n*

  2. **矩阵Σ**是一个形似“对角”矩阵

     - 其中**D是一个*r x r*的对角矩阵**，**r是矩阵A的秩**，也是**矩阵A非零奇异值的数目**
     - 矩阵D的对角线元素是A的**前r个奇异值**，通常按**降序排列**

     $$
     Σ=\left[
     \begin{matrix}
     D & \pmb{0} \\
     \pmb{0} & \pmb{0} \\
     \end{matrix}
     \right]
     $$

  3. **矩阵V**是*n x n*的正交矩阵，V的列称为A的**右奇异向量**，求解方法很简单：

     - 求矩阵A^T^A的特征值和特征向量{**v~1~**,...,**v~n~**}
     - 由于A^T^A是对称矩阵，其特征向量必定是**单位正交基**，这跟**特征值的非零数目无关**
     - V=[**v~1~**,...,**v~n~**]对应D中降序排列的特征值

  4. **矩阵U**是*m x m*的正交矩阵，U的列称为A的**左奇异向量**

     - U矩阵构造相对麻烦，依赖向量A**v~i~**

     - 对A**v~i~**单位化
       $$
       \pmb{u_i}=\frac{1}{\parallel A\pmb{v_i} \parallel}A\pmb{v_i}
       $$
       如果r是A^T^A矩阵的非零特征值数目，也是矩阵A的秩，可以得到一个**正交基**{**u~1~**,...,**u~r~**}

     - {**u~1~**,...,**u~r~**}扩充到{**u~1~**,...,**u~m~**}

       方法很简单，原来的{**u~1~**,...,**u~r~**}不变，扩充的向量只需要满足条件：**正交且单位化**，设扩充向量**x**，求方程：u~i~^T^ **x**=0

       扩充的**x**向量只是跟**u~i~**正交，内部之间需要再正交单位化，需要用到**格拉姆-施密特方法**

# 10 Python求解

## 10.1 PyTorch中矩阵乘法、内积、逐元素乘法

- 广播机制

  - NumPy、PyTorch等科学计算库中的数组（张量）操作中常见的特性

  - 使不同尺寸的张量具备可操作性
  - 自动扩展张量（数组）

  - 两种情况：

    1. 两个张量A和B在**某个维度上的尺寸不一样**。但是其中一个张量A**在这个维度上的尺寸是1**，**如果还有其他维度，其他维度方向的尺寸必须相同**。广播机制会将张量A那个维度上的尺寸扩展到和张量B一样。比如：

       ```python
       import torch
       #张量A是2*3
       A=torch.tensor([[0,1,2],
         [3,4,5]])
       
       #张量B是1*3
       B=torch.tensor([[1,2,3]])
       
       #torch中一些运算可以触发广播机制，比如+
       C=A+B
       
       '''
       C=tensor([[1, 3, 5],
               [4, 6, 8]])
       1.这里张量A和B尺寸不一样，一般来说是不能用+运算
       2.这里的广播机制触发了，就是因为A和B的尺寸虽然不一样，但是在一个维度上尺寸相同，在另一个不同尺寸的维度上A的尺寸是1
       3.广播机制使得A会自动采用复制的方法扩展为2*3,实际参与计算中的A=[[1,2,3]，[1,2,3]]
       '''
       
       ------------------------------------
       #广播机制并不只在一个维度上扩展
       #张量D是1*3*4
       D=torch.arange(12).reshape(1,3,4)
       
       #张量B是4*1*4
       E=torch.arange(16).reshape(4,1,4)
       '''
       D和E向量同样能触发广播机制
       1.D和E在第三维度上尺寸相同都是4
       2.第一和第二维度上尺寸不同，但是都有1
       '''
       
       ------------------------------------
       # 创建一个形状为(3, 1)的张量
       F = torch.tensor([[1], [2], [3]])
       
       # 创建一个形状为(1, 2)的张量
       G = torch.tensor([[4, 5]])
       '''
       F和G向量同样能触发广播机制
       1.第一和第二维度不一样，但是这两个维度上都有尺寸1
       2.没有其他维度
       '''
       ```

    2. 两个张量的维度不一样。将**较小的维度张量**扩展到与**较大维度张量**的维度数相等，办法是通过**在前面添加尺寸为1的维度**来实现

       ```python
       import torch
       
       # 创建一个标量（0维张量）
       A = 10
       
       # 创建一个形状为(3, 2)的张量
       B = torch.tensor([[1, 2], [3, 4], [5, 6]])
       
       # 使用广播机制将标量B添加到张量A的每个元素
       C = A + B
       '''
       A和B向量会触发广播机制
       1.维度都不一样
       2.小维度张量A扩展到大维度张量B
       3.在张量A维度上添加尺寸1，这里需要添加两次，A尺寸：1*1
       4.可以继续触发第一种广播机制，A尺寸1*1，B尺寸:3*2
       5.第一种情况的广播机制下，A会复制扩展为3*2
       6.实际参与计算中的A=[[10,10],[10,10],[10,10]]
       最终C=tensor([[11, 12],
                    [13, 14],
               	 [15, 16]])
       '''
       ```

       

- 逐元素乘法

  1. 两个**相同形状**的Pytorch张量,对应位置上的元素向量
  2. `torch.mul()`或者`*`来执行逐元素乘法
  3. 两者都支持广播机制，执行逐元素的乘法操作方面是完全一样的，它们具有相同的功能和语义。

  ```python
  import torch
  
  # 创建一个形状为(3, 2)的张量
  A = torch.tensor([[1, 2], [3, 4], [5, 6]])
  
  # 创建一个形状为(1, 2)的张量
  B = torch.tensor([[2, 3]])
  
  # 使用广播机制进行逐元素乘法
  C = torch.mul(A, B)
  # 使用广播机制进行逐元素乘法
  D = A * B
  ```

- **内积**，也叫**点乘，点积**

  - `torch.dot()` 函数来计算内积

  - **不支持广播机制**

    ```python
    import torch
    
    # 创建两个一维张量
    tensor1 = torch.tensor([1, 2, 3])
    tensor2 = torch.tensor([4, 5, 6])
    
    # 计算内积
    dot_product = torch.dot(tensor1, tensor2)
    ```

- 矩阵乘法

  - `三种：torch.mm()`、 `torch.matmul()` 、`@`
  - 其中`@` 运算符和 `torch.matmul()` 函数在执行矩阵乘法操作时是完全等同的。
  - 注意：**`torch.matmul()` 和`@`在计算两个向量（一维张量）的时候，矩阵乘法变成了内积**
  - 注意：**`torch.matmul()` 和`@`支持广播机制**，但是只支持广播机制中**增加维度的情况**，也就是**一维张量和二维张量矩阵乘法的时候**，会在一维张量前面增加尺寸1，从而两个二位张量实现矩阵乘法。
  - 注意：**`torch.matmul()` 和`@`并不支持第一种广播机制**，比如`3*2`矩阵和`1*4`矩阵相乘，并不会将`1*4`矩阵扩展为`2*4`
  - `torch.mm()`是纯粹的矩阵乘法，**不支持广播机制，只支持矩阵（二维张量），并检查尺寸是否复合规则**

  ```python
  import torch
  
  # 创建形状为(3, 2)的张量
  A = torch.tensor([[1, 2], [3, 4], [5, 6]])
  
  # 创建形状为(2, 2)的张量
  B = torch.tensor([[2, 3], [4, 5]])
  
  # 三种方法，正常的矩阵乘法
  result = A @ B
  result =torch.matmul(A,B)
  result =torch.mm(A,B)
  
  ---------------------------------------------------
  # 创建两个一维张量
  tensor1 = torch.tensor([1, 2, 3])
  tensor2 = torch.tensor([4, 5, 6])
  
  # 矩阵乘法变成了内积
  dot_product = torch.matmul(tensor1, tensor2)
  dot_product = tensor1@tensor2
  dot_product = torch.mm(tensor1, tensor2) #报错
  
  ----------------------------------
  # 创建一个形状为1维张量
  A = torch.tensor([2, 3])
  # 创建一个形状为(2, 3)的张量
  B = torch.tensor([[1, 2,3],[4, 5,6]])
  
  result = A @ B
  result =torch.matmul(A,B)
  result =torch.mm(A,B)   #报错
  '''
  触发广播机制
  A张量是1维的，扩展为2维，尺寸1*2
  '''
  
  --------------------------------------------------
  #张量A是3*2
  A=torch.arange(6).reshape(3,2)
  #张量B是1*4
  B=torch.arange(4).reshape(1,4)
  
  #三种情况都在报错
  result = A @ B
  result =torch.matmul(A,B)
  result =torch.mm(A,B)  
  ```

  - `torch.bmm()`是批量矩阵乘法，**不支持广播机制**。在张量概念里，**根据张量的维数区分数，向量，矩阵。0维张量是数，1维张量是向量，2维张量是矩阵，3维张量就是批量矩阵**

    ```python
    import torch
    #三维张量，第一维代表批量batch
    A=torch.arange(6*2).reshape(2,2,3)
    B=torch.arange(6*2).reshape(2,3,2)
    C=torch.bmm(A,B)
    '''
    
    C = tensor([[[ 10,  13],
             	 [ 28,  40]],
    
            	[[172, 193],
             	 [244, 274]]])
    '''
    ```

## 10.2 代码求行列式

- 不准确性
  - 计算行列式可以受到数值精度的限制，因此有时可能会出现不准确的结果
  - **通过行列式判断矩阵是否可逆，一定要谨慎，因为很大概率一个不可逆矩阵，代码求行列式不等于0**

- **numpy**

  - 示例矩阵的实际行列式等于0，代码计算结果有误

  ```python
  import numpy as np
  
  # 创建一个方阵
  matrix = np.array([[1, 2, 3],
                     [4, 5, 6],
                     [7, 8, 9]])
  
  # 计算行列式
  det = np.linalg.det(matrix)
  print("矩阵的行列式:", det) #6.66133814775094e-16
  ```

- **pytorch**

  - **准确度高于numpy**

  ```python
  import torch
  
  # 创建一个方阵
  matrix = torch.tensor([[1.0, 2.0, 3.0],
                         [4.0, 5.0, 6.0],
                         [7.0, 8.0, 9.0]], dtype=torch.float64)
  
  # 计算行列式
  det = torch.det(matrix)
  print("矩阵的行列式:", det) #0
  ```

  ```python
  
  ```

## 10.3 逆矩阵

- numpy和pytorch库

  ```python
  import numpy as np
  
  #创建矩阵
  matrix = np.array([[2, 1], [5, 3]])
  matrix = np.matrix(matrix)
  
  #求逆矩阵
  inverse_matrix = np.linalg.inv(matrix)
  
  #判断矩阵是否可逆
  det = np.linalg.det(matrix)
  if det != 0:
      inverse_matrix = np.linalg.inv(matrix)
      print("逆矩阵：\n", inverse_matrix)
  else:
      print("矩阵不可逆")
  
  --------------------------------------
  import torch
  
  # 创建一个矩阵
  matrix = torch.tensor([[2.0, 1.0], [5.0, 3.0]])
  
  # 求逆矩阵
  inverse_matrix = torch.inverse(matrix)
  
  print("逆矩阵：")
  print(inverse_matrix)
  ```

## 10.4 矩阵的秩

- Numpy

  ```python
  import numpy as np
  
  # 创建一个矩阵
  matrix = np.array([[1, 2, 3],
                    [4, 5, 6],
                    [7, 8, 9]])
  
  # 计算矩阵的秩
  rank = np.linalg.matrix_rank(matrix)
  print("矩阵的秩:", rank)
  ```

- pytorch

  ```python
  import torch
  
  # 创建一个矩阵
  matrix = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])
  
  # 计算矩阵的秩
  rank = torch.matrix_rank(matrix)
  
  print("矩阵的秩为:", rank)
  ```

## 10.5 特征值分解： 特征值和特征向量

- 注意：

  1. **矩阵的特征值是固定的，特征向量并不固定**

  2. numpy和pytorch中，**只求特征值**的方法：`numpy.linalg.eigvals`和`torch.linalg.eigvals`

     ```python
     import numpy as np
     
     # 创建一个矩阵
     matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
     
     # 使用 NumPy 的eigvals 函数来计算特征值
     eigenvalues = np.linalg.eigvals(matrix)
     
     print("特征值:")
     print(eigenvalues)
     #输出：[ 1.61168440e+01 -1.11684397e+00 -4.22209278e-16]
     
     
     import torch
     # 创建一个矩阵
     matrix = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])
     
     # 使用 PyTorch 的 eigvals` 函数来计算特征值
     eigenvalues = torch.linalg.eigvals(matrix)
     
     print("特征值:")
     print(eigenvalues)
     #输出：tensor([ 1.6117e+01+0.j, -1.1168e+00+0.j, -1.2253e-07+0.j])
     
     print(eigenvalues.real)
     #输出：tensor([ 1.6117e+01, -1.1168e+00, -1.2253e-07])
     ```

  3. `np.linalg.eig`和`torch.linalg.eig`求特征值和特征向量，**本质是特征值分解，也就是对角化，使用函数的前提就是可以对角化，有n个线性无关的特征向量**

     - `eigenvalues,eigenvectors= np.linalg.eig`和`eigenvalues,eigenvectors = torch.linalg.eig`输出两个，分别是**特征值**和**特征向量组成的矩阵**，并不是直接输出特征向量，而是：
       $$
       A=PDP^{-1}
       $$
       中的`eigenvectors = P`

     - `eigenvectors`是**单位特征向量**组成

     ```python
     import numpy as np
     
     # 创建一个对称矩阵
     symmetric_matrix = np.array([[1, 2, 3], [2, 4, 5], [3, 5, 6]])
     
     # 使用 NumPy 的 eig 函数来进行特征值分解
     eigenvalues, eigenvectors = np.linalg.eigh(symmetric_matrix)
     
     print("特征值:")
     print(eigenvalues)
     
     print("特征分解矩阵:")
     print(eigenvectors)
     
     --------------------------------------------------
     '''
     两种验证：
     1.Ax=λx
     2.A=PDP-1
     '''
     
     #Ax
     result1 = np.dot(symmetric_matrix, eigenvectors[:,0])
     #λx
     result2 = eigenvalues[0] * eigenvectors[:,0]
     # 使用 np.allclose 进行近似相等性比较
     print(np.allclose(result1, result2))
     
     #特征值构建对角矩阵
     diagonal_matrix = np.diag(eigenvalues)
     #求逆矩阵
     inverse_eigenvectors = np.linalg.inv(eigenvectors)
     #PDP-1和A
     print(np.allclose(eigenvectors@diagonal_matrix@inverse_eigenvectors, symmetric_matrix))
     ```

     - 注意：**pytorch中计算特征值和特征向量都是复数形式**，包括`torch.linalg.eig`和`torch.linalg.eigvals`，在张量后面加上**real方法**，可以只剩实数

     ```python
     import torch
     
     # 创建一个对称矩阵
     symmetric_matrix = torch.tensor([[1.0, 2.0, 3.0], [2.0, 4.0, 5.0], [3.0, 5.0, 6.0]])
     
     # 使用 PyTorch 的 eig 函数来进行特征值分解
     eigenvalues, eigenvectors = torch.linalg.eig(symmetric_matrix)
     
     #复数形式到实数形式
     eigenvalues = eigenvalues.real
     eigenvectors = eigenvectors.real
     
     print("特征值:")
     print(eigenvalues)
     
     print("特征分解矩阵:")
     print(eigenvectors)
     
     --------------------------------------------------
     '''
     两种验证：
     1.Ax=λx
     2.A=PDP-1
     '''
     
     #Ax
     result1 = torch.matmul(symmetric_matrix, eigenvectors[:,0])
     #λx
     result2 = eigenvalues[0] * eigenvectors[:,0]
     # 使用 torch.allclose 进行近似相等性比较
     print(torch.allclose(result1, result2))
     
     #特征值构建对角矩阵
     diagonal_matrix = torch.diag(eigenvalues)
     #求逆矩阵
     inverse_eigenvectors = torch.inverse(eigenvectors)
     print(torch.allclose(eigenvectors@diagonal_matrix@inverse_eigenvectors, symmetric_matrix))
     ```

  4. 如果矩阵不满足对角化的条件：**n个线性无关的特征向量**，`np.linalg.eig`和`torch.linalg.eig`还是能分解，但是eigenvectors会有重复的特征向量，此时：**A≠PDP^-1^**

     ```python
     import torch
     
     # 创建一个不能对角化的矩阵
     symmetric_matrix = torch.tensor([[2.0, 4.0, 3.0], [-4.0, -6.0, -3.0], [3.0, 3.0, 1.0]])
     
     # 使用 PyTorch 的 eig 函数来进行特征值分解
     eigenvalues, eigenvectors = torch.linalg.eig(symmetric_matrix)
     
     #复数形式到实数形式
     eigenvalues = eigenvalues.real
     eigenvectors = eigenvectors.real
     
     print("特征值:")
     print(eigenvalues)
     
     print("特征分解矩阵:")
     print(eigenvectors)
     
     #Ax
     result1 = torch.matmul(symmetric_matrix, eigenvectors[:,0])
     #λx
     result2 = eigenvalues[0] * eigenvectors[:,0]
     # 使用 torch.allclose 进行近似相等性比较
     print(torch.allclose(result1, result2))
     
     #特征值构建对角矩阵
     diagonal_matrix = torch.diag(eigenvalues)
     #求逆矩阵
     inverse_eigenvectors = torch.inverse(eigenvectors)
     print(torch.allclose(eigenvectors@diagonal_matrix@inverse_eigenvectors, symmetric_matrix))#false
     ```

  5. 另外还有，`numpy.linalg.eigh`和`torch.linalg.eigh`**专注于埃尔米特矩阵（Hermitian matrix）或者对称矩阵的特征值分解**，如果输入的矩阵不是这两种矩阵，不会报错，但是分解结果是错误的

## 10.6 对称矩阵的正交对角化

- ` numpy.linalg.eigh`和`torch.linalg.eigh`专门针对对称矩阵的分解
  - `np.linalg.eig`和`torch.linalg.eig`对于对称矩阵，其实也可以分解
  - 注意：**如果对角化的矩阵是对称的，其实求得的特征向量是正交的，不需要再格拉姆-施密特方法**

## 10.7 格拉姆施密特方法

格拉姆-施密特正交化方法需要根据公式自己编写代码来实现，没有现成的方法

```python
import torch

# 创建一组线性无关的向量
v1 = torch.tensor([1.0, 2.0, 3.0])
v2 = torch.tensor([2.0, 3.0, 4.0])
v3 = torch.tensor([3.0, 4.0, 5.0])

# 初始化正交化后的向量
orthogonalized_v1 = v1.clone()
orthogonalized_v2 = v2 - torch.dot(orthogonalized_v1, v2) / torch.dot(orthogonalized_v1, orthogonalized_v1) * orthogonalized_v1
orthogonalized_v3 = v3 - torch.dot(orthogonalized_v1, v3) / torch.dot(orthogonalized_v1, orthogonalized_v1) * orthogonalized_v1 - \
                   torch.dot(orthogonalized_v2, v3) / torch.dot(orthogonalized_v2, orthogonalized_v2) * orthogonalized_v2

# 归一化
orthonormal_v1 = orthogonalized_v1 / torch.norm(orthogonalized_v1)
orthonormal_v2 = orthogonalized_v2 / torch.norm(orthogonalized_v2)
orthonormal_v3 = orthogonalized_v3 / torch.norm(orthogonalized_v3)

print("Orthogonalized and normalized vectors:")
print(orthonormal_v1)
print(orthonormal_v2)
print(orthonormal_v3)
```



## 10.8 LU分解

## 10.9 QR分解

## 10.10 奇异值分解

- numpy

  ```python
  import numpy as np
  
  # 创建一个矩阵
  matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
  
  # 进行SVD分解
  U, S, Vt = np.linalg.svd(matrix)
  
  # U是左奇异矩阵，S是奇异值数组，V是右奇异矩阵
  print("左奇异矩阵 U:")
  print(U)
  print("奇异值 S:")
  print(S)
  print("右奇异矩阵 Vt:")
  print(Vt)
  
  ---------------------------------------------------------
  '''
  验证：A=UΣVT
  '''
  # 构造对角矩阵Σ
  Σ = np.zeros_like(matrix, dtype=float)
  Σ[:min(matrix.shape), :min(matrix.shape)] = np.diag(S)
  
  # 验证A = UΣV^T
  reconstructed_matrix = U.dot(Σ).dot(Vt)
  print("Original Matrix A:")
  print(matrix)
  print("Reconstructed Matrix UΣV^T:")
  print(reconstructed_matrix)
  print(np.allclose(matrix,reconstructed_matrix))
  ```

- pytorch

  - 精度原因，没有完全能验证对上
  
  ```python
  import torch
  
  # 创建一个PyTorch张量
  matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float32)
  
  # 进行SVD分解
  U, S, Vt = torch.svd(matrix)
  # U是左奇异矩阵，S是奇异值数组，V是右奇异矩阵
  print("左奇异矩阵 U:")
  print(U)
  print("奇异值 S:")
  print(S)
  print("右奇异矩阵 Vt:")
  print(Vt)
  
  ----------------------------------------------------
  '''
  验证：A=UΣVT
  '''
  # 构造对角矩阵Σ
  Σ = torch.zeros_like(matrix, dtype=torch.float32)
  Σ[:min(matrix.shape[0], matrix.shape[1]), :min(matrix.shape[0], matrix.shape[1])] = torch.diag(S)
  
  # 验证A = UΣV^T
  reconstructed_matrix = torch.mm(torch.mm(U, Σ), Vt)
  print("Original Matrix A:")
  print(matrix)
  print("Reconstructed Matrix UΣV^T:")
print(reconstructed_matrix)
  ```

  
  
  



